\documentclass[../main.tex]{subfiles}

\begin{document}
\chapter{Operators on Complex Vector Spaces}

\begin{plainblackenv}
Standing Assumption
\tcblower\par
\begin{itemize}
\item $\mathbb{F}$ denotes $\mathbb{R}$ or $\mathbb{C}$.
\item $V$ denotes finite dimensional vector spaces.
\end{itemize}
\end{plainblackenv}

\section{Generalized Eigenvectors and Nilpotent Operators}

\subsection{Null Spaces of $T^k$}

\begin{proposition}{Sequences of Increasing Null Spaces}{Sequences of Increasing Null Spaces}
Suppose $T\in \mathscr{L}(V)$, then
\begin{equation*}
\left\{ 0 \right\} = \snull T^0 \subseteq \snull T^1 \subseteq \cdots \subseteq \snull T^k \subseteq \cdots 
\end{equation*}
\end{proposition}
\begin{proof}
If $v\in \snull T^k$, then $T^kv=0$, then $T^{k+1}v=0$, then $v\in \snull T^{k+1}$.
\end{proof}

\begin{proposition}{Equality in the Sequence of Null spaces}{Equality in the Sequence of Null spaces}
Suppose $T\in \mathscr{L}(V)$ and $m\in \mathbb{N}$ such that
\begin{equation*}
\snull T^m = \snull T^{m+1}
\end{equation*}
Then $\forall k \in \mathbb{N}$, we have
\begin{equation*}
	\snull T^{m+k} = \snull T^m
\end{equation*}
\end{proposition}
\begin{proof}
We want to prove $\snull T^{m+k} = \snull T^{m+k+1}$. We already know that $\snull T^{m+k} \subseteq \snull T^{m+k+1}$. Suppose $v\in \snull T^{m+k+1}$, then
\begin{equation*}
T^{m+1}(T^kv) = 0
\end{equation*}
then $T^k v \in \snull T^{m+1} = \snull T^m$, so $v\in \snull T^{m+k}$.
\end{proof}
This result shows that the sequence of null spaces will tend to stable. A direct result is that the null spaces can grow only $\dim V$ times, which increase $1$ dimension every step.

\begin{proposition}{Null spaces Stop growing}{Null spaces Stop growing}
Suppose $T\in \mathscr{L}(V)$, then
\begin{equation*}
\snull T^{\dim V} = \snull T^{\dim V+1} = \cdots 
\end{equation*}
\end{proposition}

It is not always true that $V = \snull T \oplus \range T$ for $\forall T\in \mathscr{L}(V)$, however, the following proposition is a useful substitute.
\begin{theorem}{$V = \snull T^{\dim V} \oplus \range T^{\dim V}$}{V  snull Tdim V oplus range Tdim V}
Suppose $T\in \mathscr{L}(V)$, then
\begin{equation*}
V = \snull T^{\dim V} \oplus \range T^{\dim V}
\end{equation*}
\end{theorem}
\begin{proof}
We already have $\dim V = \dim \snull T^{\dim V} + \dim \range T^{\dim V}$. So we only need to prove
\begin{equation*}
	(\snull T^n) \cap (\range T^n) = \left\{ 0 \right\}
\end{equation*}
Where $n = \dim V$. Suppose $v\in (\snull T^n) \cap (\range T^n)$, then $T^n v=0$. Also $\exists u\in V, v = T^nu$, so $T^{2n}u=0$, thus $u\in \snull T^{2n} = \snull T^n$. Therefore $v = T^nu = 0$.
\end{proof}

Accordingly, we have a similar result for range.
\begin{proposition}{The range of powers}{The range of powers}
Suppose $T\in \mathscr{L}(V)$, then
\begin{itemize}
\item The range sequence:
\begin{equation*}
V = \range T^0 \supseteq \range T^1 \supseteq \cdots 
\end{equation*}
\item If $m\in \mathbb{N}, \range T^m = \range T^{m+1}$, then $\forall k\in \mathbb{N}$, we have
	\begin{equation*}
	\range T^{m+k} = \range T^m
	\end{equation*}
\item $\displaystyle \range T^{\dim V} = \range T^{\dim V+1} =\cdots $.
\end{itemize}
\end{proposition}
The proof is similar.

\begin{proposition}{Null Range Relation of Powers}{Null Range Relation of Powers}
Suppose $T\in \mathscr{L}(V)$, and $n\in \mathbb{N}$, then
\begin{equation*}
\snull T^m = \snull T^{m+1} \Leftrightarrow \range T^m = \range T^{m+1}.
\end{equation*}
\end{proposition}
\begin{proof}
Using the same dimension would do.
\end{proof}

\subsection{Generalized Eigenvectors}

Some operator do not have enough eigenvectors to fully describe its behavior. When we try to decompose $V$ into invariant subspaces
\begin{equation*}
V = V_1 \oplus \cdots \oplus V_m
\end{equation*}
The simplest approach is to decompose into one-dimensional spaces, which can only be done when there is an eigenvector basis. That is,
\begin{equation*}
V = E(\lambda_1, T) \oplus \cdots \oplus E(\lambda_m,T).
\end{equation*}
where $\lambda_1, \ldots ,\lambda_m$ are the distinct eigenvalues of $T$. (Some of $E(\lambda_k,T)$ may have larger dimension.)

\begin{definition}{Generalized Eigenvector}{Generalized Eigenvector}
Suppose $T\in \mathscr{L}(V)$, and $\lambda$ is an eigenvalue of $T$. A vector $v\in V$ is called a generalized eigenvector of $T$ corresponding to  $\lambda$ if $v\neq 0$ and
\begin{equation*}
	(T-\lambda I)^kv = 0
\end{equation*}
for some $k\in \mathbb{Z}_+$.
\end{definition}
Well we have $v$ is a generalized eigenvector of $T$ iff $v\neq 0$ and 
\begin{equation*}
	(T-\lambda I)^{\dim V} = 0
\end{equation*}

\begin{theorem}{A basis of Generalized Eigenvectors}{A basis of Generalized Eigenvectors}
Suppose $\mathbb{F}=\mathbb{C}$ and $T\in \mathscr{L}(V)$. Then there is a basis of $V$ consisting of generalized eigenvectors of $T$.
\end{theorem}
\begin{proof}
Let $n=\dim V$, and use induction on $n$. First the result holds for $n=1$. Suppose $n > 1$ and the result holds for all smaller $n$. Let $\lambda$ be an eigenvalue of $T$ (using the hypothesis $\mathbb{F}=\mathbb{C}$ ), then as
\begin{equation*}
V = \snull (T-\lambda I)^n \oplus \range (T-\lambda I)^n
\end{equation*}

If $\snull (T-\lambda I)^n = V$, then every nonzero $v\in V$ is a generalized eigenvector of $T$.

If $\snull (T-\lambda I)^n \neq V$, then $\range (T-\lambda I)^n \neq \left\{ 0 \right\}$, and $\snull (T-\lambda I)^n \neq \left\{ 0 \right\}$ for  $\lambda$ is an eigenvalue. Thus
\begin{equation*}
0 < \dim \range (T-\lambda I)^n < n
\end{equation*}

As $\range (T- \lambda I)^n$ is invariant under $T$. We let $S = \mathscr{L} \left(\range (T- \lambda I)^n\right)$ be the restriction of $T$ onto $\range (T- \lambda I)^n$. Then by induction hypothesis to $S$, we have there is a generalized eigenvector basis for $S$ (also $T$) of $\range (T- \lambda I)^n$. Joining the basis of  $\range (T- \lambda I)^n$ and a basis of $\snull (T-\lambda I)^n$.
\end{proof}

\begin{remark}
This result does not hold for $\mathbb{F}=\mathbb{R}$, as there may not be any eigenvalue initially.
\end{remark}

\begin{theorem}{A Basis of Generalized Eigenvectors, Revised}{A Basis of Generalized Eigenvectors Revised}
Suppose $T\in \mathscr{L}(V)$, then there is a basis of $V$ consisting of generalized eigenvectors of $T$ iff the minimal polynomial of $T$ equals $(z-\lambda_1) \cdots (z-\lambda_m)$ for some $\lambda_1, \ldots ,\lambda_m \in \mathbb{F}$. Which is equivalent to $T$ has an upper-triangular form.
\end{theorem}
\begin{proof}
	The proof of the ``if'' part is the same as above, for $T$ has an eigenvalue.
\end{proof}

\begin{theorem}{Generalized Eigenvector Corresponds to one Eigenvalue}{Generalized Eigenvector Corresponds to one Eigenvalue}
Suppose $T\in \mathscr{L}(V)$, theneach generalized eigenvector of $T$ Corresponds to only one eigenvalue of $T$.
\end{theorem}
\begin{proof}
Suppose $v\in V$ is a generalized eigenvector corresponds to $\lambda$ and $\alpha$ of $T$. Let $m$ be the smallest number such that $(T-\alpha I)^mv=0$, and $n=\dim V$. Then
\begin{equation*}
\begin{aligned}
	0 &= (T-\lambda I)^nv\\
	  &=\left((T-\alpha I) + (\alpha-\lambda)I\right)^nv\\
	  &=\sum_{k=0}^{n} b_k (\alpha-\lambda)^{n-k}(T-\alpha I)^kv
\end{aligned}
\end{equation*}
Where $b_0=1$. Applying $(T-\alpha I)^{m-1}$ to both sides we have
\begin{equation*}
0 = (\alpha-\lambda)^n(T-\alpha I)^{m-1}v
\end{equation*}
Thus $\alpha=\lambda$.
\end{proof}

\begin{theorem}{Linear Independent Generalized Eigenvectors}{Linear Independent Generalized Eigenvectors}
Suppose $T\in \mathscr{L}(V)$, then every list of generalized eigenvectors corresponding to distinct eigenvalues are linear independent.
\end{theorem}
\begin{proof}
Similar to the way we prove eigenvectors are linear independent. Let $m$ be the smaller number that $v_1, \ldots ,v_m$ corresponding to distinct $\lambda_1, \ldots ,\lambda_m$ are linear dependent. We have $m \geq 2$ and $a_1, \ldots ,a_m\in \mathbb{F}$ such that
\begin{equation*}
a_1v_1 +\ldots +a_mv_m=0.
\end{equation*}
None of $a_i=0$ for the minimality of $m$. Applying $(T-\lambda_m I)^n$ to both sides we have
\begin{equation*}
a_1(T-\lambda_m)^nv_1+\cdots + a _{m-1}(T-\lambda_m I)^n v_{m-1} = 0
\end{equation*}
As $(T-\lambda_m I)^n v_k \neq 0$ according to theorem \ref{thm:Generalized Eigenvector Corresponds to one Eigenvalue}, then $(T-\lambda_m)^n v_k$ is a smaller linear independent list, contradicts.
\end{proof}

\subsection{Nilpotent Operators}
\begin{definition}{Nilpotent Operators}{Nilpotent Operators}
	An operator $T\in \mathscr{L}(V)$ is a nilpotent operator if $\exists k\in \mathbb{N}, T^k=0$.
\end{definition}
Thus, an operator is nilpotent iff every nonzero vector in $V$ is a generalized eigenvector of $T$ corresponding to the eigenvalue $0$.

\begin{proposition}{An upper limit of the power of a nilpotent operator}{An upper limit of the power of a nilpotent operator}
Suppose $T\in \mathscr{L}(V)$ is nilpotent, then $T^{\dim V}=0$.
\end{proposition}
\begin{proof}
	If $\exists k\in \mathbb{Z}_+, T^k=0$, then $\snull T^k=V$, then $T^{\dim V} = V$, then $T^{\dim V}=0$.
\end{proof}

As all generalized eigenvector corresponds to eigenvalue $0$, then we say that there are no eigenvalue other than $0$ for a nilpotent operator.

\begin{theorem}{Eigenvalues of Nilpotent Operators}{Eigenvalues of Nilpotent Operators}
Suppose $T\in \mathscr{L}(V)$, then
\begin{itemize}
\item If $T$ is nilpotent, then $0$ is the only eigenvalue of $T$.
\item If $\mathbb{F}=\mathbb{C}$ and $0$ is the only eigenvalue of $T$, then $T$ is nilpotent.
\end{itemize}
\end{theorem}
\begin{proof}
\begin{itemize}
\item As $T^m=0$ for some $m\in \mathbb{Z}_+$, then $T$ is not injective, thus  $0$ is an eigenvalue of $T$.

	If $Tv = \lambda v$, then $T^mv=\lambda^mv = 0$, thus $\lambda=0$.
\item Suppose $\mathbb{F}=\mathbb{C}$ and $0$ is the only eigenvalue of $T$, then the minimal polynomial of $T$ equals $z^m$, thus $T^m=0$ and $T$ is nilpotent.
\end{itemize}
\end{proof}

\begin{remark}
We already know a class of nilpotent operators has the matrix form
\begin{equation*}
\begin{pmatrix}
	0&&*\\
	 &\ddots &\\
	0&&0
\end{pmatrix}
\end{equation*}
We shall prove that all nilpotent operators has this form.
\end{remark}

\begin{theorem}{Minimal Polynomial and Matrix of Nilpotent Operators}{Minimal Polynomial and Matrix of Nilpotent Operators}
Suppose $T\in \mathscr{L}(V)$, then the following are equivalent.
\begin{itemize}
\item $T$ is nilpotent.
\item The minimal polynomial of $T$ is $z^m$ for some $m \geq 0$.
\item There is a basis of $V$ such that the matrix of $T$ is
\begin{equation*}
\begin{pmatrix}
	0&&*\\
	 &\ddots &\\
	0&&0
\end{pmatrix}
\end{equation*}
\end{itemize}
\end{theorem}
\begin{proof}
\begin{itemize}
\item Suppose $T$ is nilpotent, then $T^n=0$ for some $n\in \mathbb{Z}_+$, then the minimal polynomial is a factor of $z^n$.
\item By the minimal polynomial, we have a upper-triangular form, also the diagonal is the roots of the minimal polynomial, then we get the result.
\end{itemize}
\end{proof}

\section{Generalized Eigenspace Decomposition}
\subsection{Generalized Eigenspaces}
\begin{definition}{Generalized Eigenspaces}{Generalized Eigenspaces}
Suppose $T\in \mathscr{L}(V)$, and $\lambda\in \mathbb{F}$, then the generalized eigenspace of $T$ corresponding to $\lambda$ is defined
\begin{equation}
G(\lambda, T) = \left\{ v\in V: \exists k\in \mathbb{Z}_+,(T-\lambda I)^kv = 0 \right\}
\end{equation}
Or equivalently,
\begin{equation*}
G(\lambda,T) = \snull (T-\lambda I)^{\dim V}.
\end{equation*}
\end{definition}
It is easy to see that $E(\lambda,T) \subseteq G(\lambda, T)$.

\begin{theorem}{Generalized Eigenspace Decomposition}{Generalized Eigenspace Decomposition}
Suppose $\mathbb{F}=\mathbb{C}$ and $T\in \mathscr{L}(V)$. Let $\lambda_1, \ldots ,\lambda_m$ be distinct eigenvalues of $T$. Then
\begin{enumerate}
	\item $(G,\lambda_k, T)$ is invariant under $T$.
	\item $(T-\lambda_k I)|_{G(\lambda_k,T)}$ is nilpotent.
	\item $V = G(\lambda_1,T) \oplus \cdots \oplus G(\lambda_m,T)$.
\end{enumerate}
\end{theorem}
\begin{proof}
\begin{enumerate}
	\item As $G(\lambda_k, T) = \snull (T-\lambda_kI)^{\dim V}$.
	\item Obvious.
	\item As generalized eigenvectors corresponding to different eigenvalues are linear independent, the $\bigoplus G(\lambda_k,T)$ is a direst sum. Also there is a generalized eigenvector basis.
\end{enumerate}
\end{proof}

\subsection{Multiplicity of an Eigenvalue}

\begin{definition}{Multiplicity}{Multiplicity}
Suppose $T\in \mathscr{L}(V)$, the multiplicity of an eigenvalue $\lambda$ of $T$ is defined to be $\dim G(\lambda,T)$.
\end{definition}

We have a direct result:
\begin{proposition}{Sum of Multiplicity}{Sum of Multiplicity}
Suppose $\mathbb{F}=\mathbb{C}$ and $T\in \mathscr{L}(V)$, then the sum of multiplicity of all eigenvalues is $\dim V$,
\end{proposition}
\begin{proof}
This follows directly from the generalized eigenvector decomposition.
\end{proof}

\begin{remark}
\textbf{Algebraic Multiplicity and Geometric Multiplicity}

Traditional definition of algebraic multiplicity involves the use of determinants. But now we see that it also has a geometric meaning.

\begin{itemize}
	\item \textbf{Algebraic Multiplicity}: $\dim G(\lambda, T) = \dim \snull (T-\lambda I)^{\dim V}$.
	\item \textbf{Geometric Multiplicity}: $\dim E(\lambda, T) = \dim \snull (T-\lambda I)$.
\end{itemize}

If $V$ is an inner product space, $T\in \mathscr{L}(V)$ is normal, and $\lambda$ is an eigenvalue of $T$, then the algebraic multiplicity equals the geometric multiplicity. (As $T-\lambda I$ is normal, write it as a diagonal matrix).
\end{remark}

\begin{definition}{Characteristic Polynomial}{Characteristic Polynomial}
Suppose $\mathbb{F}=\mathbb{C}$ and $T\in \mathscr{L}(V)$, Let $\lambda_1, \ldots ,\lambda_m$ denote the distinct eigenvalues with multiplicities $d_1, \ldots ,d_m$, then we define characteristic polynomial 
\begin{equation}
	(z-\lambda_1)^{d_1} \cdots (z-\lambda_m)^{d_m}
\end{equation}
\end{definition}

\begin{theorem}{Degree and Zeros of Characteristic Polynomial}{Degree and Zeros of Characteristic Polynomial}
Suppose $\mathbb{F}=\mathbb{C}$ and $T\in \mathscr{L}(V)$, $p$ is the characteristic polynomial. Then
\begin{itemize}
\item $\deg p = \dim V$.
\item The zeros of $p$ are the eigenvalues of $T$.
\end{itemize}
\end{theorem}

\begin{theorem}{Cayley-Hamilton Theorem}{Cayley-Hamilton Theorem}
Suppose $\mathbb{F}=\mathbb{C}$ and $T\in \mathscr{L}(V)$, $q$ is the characteristic polynomial of $T$. Then $q(T)=0$.
\end{theorem}
\begin{proof}
We have
\begin{equation*}
q(T) = (T-\lambda_1I)^{d_1}\cdots (T-\lambda_mI)^{d_m}.
\end{equation*}
As $(T-\lambda_k)^{d_k}|_{G(\lambda_k,T} = 0$, then $q(T)|_{G(\lambda_k,T)}=0$. As $V = \bigoplus G(\lambda_k,I)$ then $q(T) = 0$.
\end{proof}

\begin{proposition}{Characteristic Polynomial and the Minimal Polynomial}{Characteristic Polynomial and the Minimal Polynomial}
Suppose $\mathbb{F}=\mathbb{C}$ and $T\in \mathscr{L}(V)$, then the characteristic polynomial is a multiple of the minimal polynomial.
\end{proposition}
\begin{proof}
This follows immediately from the Cayley-Hamilton Theorem.
\end{proof}

\begin{theorem}{Multiplicity and the Diagonal}{Multiplicity and the Diagonal}
Suppose $\mathbb{F}=\mathbb{C}$ and $T\in \mathscr{L}(V)$. $v_1, \ldots ,v_n$ is a basis which $T$ has an upper-triangular matrix $\mathscr{M}$. Then the multiplicity of $\lambda$ is the number of times $\lambda$ appears on the diagonal of $\mathscr{M}$.
\end{theorem}
\begin{proof}
As all the columns that $\lambda_k\neq 0$ are linear independent (obvious), then let $d$ be the number of $\lambda_k=0$. We have
\begin{equation*}
\dim \range T \geq n-d
\end{equation*}
Thus
\begin{equation*}
\dim \snull T \leq d
\end{equation*}
As $T^n$ has diagonal $\lambda_1^n, \ldots ,\lambda_n^n$, then
\begin{equation*}
\dim \snull T^n \leq d
\end{equation*}
Let $m_{\lambda}$ be the multiplicity of $\lambda$ and $d_{\lambda}$ be the number of $\lambda$ appears on the diagonal. Replacing $T$ with $T-\lambda I$ we have
\begin{equation*}
m_{\lambda} \leq d_{\lambda}
\end{equation*}
As $\sum m_{\lambda} = \sum d_{\lambda} = n$, so $m_{\lambda} = d_{\lambda}$.
\end{proof}

\subsection{Block Diagonal Matrices}

To see that decomposing a vector space into direct sums of invariant subspaces would simplify matter, we see that the matrix form has a \emph{block-diagonal} pattern:
\begin{equation*}
\begin{pmatrix}
	A_1&&0\\
	   &\ddots &\\
	0&&A_m
\end{pmatrix}
\end{equation*}
Where $A_k$ are square matrices. This is obvious since each $A_k$ is the restraint of $T$ onto one of the invariant subspaces, mapping a part of the coordinated onto exactly the same part.

\begin{theorem}{Block Diagonal Matrices with Upper-triangular Blocks}{Block Diagonal Matrices with Upper-triangular Blocks}
Suppose $\mathbb{F}=\mathbb{C}$ and $T\in \mathscr{L}(V)$, let $\lambda_1, \ldots ,\lambda_m$ be the eigenvalues of $T$, with multiplicities $d_k$. Then there is a basis of $V$ such that $T$ has a block-diagonal form
\begin{equation*}
\begin{pmatrix}
	A_1&&0\\
	   &\ddots &\\
	0&&A_m
\end{pmatrix}
\end{equation*}
With $A_k$ a $d_k \times d_k$ upper-triangular matrix.
\end{theorem}
\begin{proof}
Combining the upper-triangular-basis of $(T-\lambda_kI)|_{G(\lambda_k,T)}$ would do.
\end{proof}

\section{Consequences of General Eigenspace Decomposition}
\subsection{Square Root of Operators}
We have seen that every positive operator has a square root. This is not true for all operators on $\mathbb{C}$.
\begin{example}{Operators that does not have a Square Root}{Operators that does not have a Square Root}
$T(z_1,z_2,z_3) = (z_2,z_3,0)$ does not have a square root.
\end{example}

\begin{proposition}{Identity + Nilpotent has a square root}{Identity + Nilpotent has a square root}
Suppose $T\in \mathscr{L}(V)$ is nilpotent. Then $I+T$ has a square root.
\end{proposition}
Our motivation of the proof comes from the Taylor series of the function $\sqrt{1+x}$.
\begin{proof}
We let the Taylor series of $\sqrt{1+x}$ to be
\begin{equation*}
\sqrt{1+x} = 1+a_1x+a_2x^2+\cdots 
\end{equation*}
Let $m$ be the smallest integer that $T_m=0$, consider the polynomial
\begin{equation*}
p(T) = I + a_1T + \cdots + a _{m-1}T^{m-1}
\end{equation*}
And we want to show that $p(T)^2=I+T$. Just solving each $a_k$ would do.
\end{proof}

\begin{theorem}{Invertible Operators over $\mathbb{C}$ have Square Roots}{Invertible Operators over mathbbC have Square Roots}
Suppose $\mathbb{F}=\mathbb{C}$ and $T\in \mathscr{L}(V)$, then $T$ has a square root.
\end{theorem}
\begin{proof}
Let $\lambda_1, \ldots ,\lambda_m$ be the distinct eigenvalue of $T$. As $(T-\lambda_kI)|_{G(\lambda_k,T)}$ is nilpotent, denote it $T_k\in \mathscr{L}(G(\lambda_k,T))$, thus $T|_{G(\lambda_k,T)} = \lambda_kI+T_k$.

Because $T$ is invertible, $\lambda_k \neq 0$ for all $k$. Thus, we write
\begin{equation*}
T|_{G(\lambda_k,T)}= \lambda_k \left(1 + \frac{T_k}{\lambda_k}\right)
\end{equation*}
We have $1 + T_k / \lambda_k$ has a square root and so do $\lambda_k$, thus $T|_{G(\lambda_k,T)}$ has a square root. Changing the diagonal block to its square root we get a square root of $T$.

(Mathematically, for each $v = u_1+\ldots +u_m$ where $u_k\in G(\lambda_k,T)$, we let $Rv = R_1u_1+\ldots +R_mu_m$ would do.)
\end{proof}

\begin{corollary}{$k^\text{th}$ root of Invertibles over $\mathbb{C}$}{ktextth root of Invertibles over mathbbC}
Suppose $\mathbb{F}=\mathbb{C}$ and $T\in \mathscr{L}(V)$, then $T$ has a $k^\text{th}$ root for all $k\in \mathbb{Z}_+$.
\end{corollary}

\subsection{Jordan Form}
We can do even better with the block-diagonal form. We shall prove that $V$ has a basis to which the matrix of $T$ has nonzero elements only on the diagonal and the line above the diagonal.

In the last section, we know that we should focus on the operator $T|_{G(\lambda_k,T)}$. This operator is nilpotent. If we can prove that every nilpotent operator has a basis to which the matrix form has nonzero elements only at the line above the diagonal, we're done!

\begin{definition}{Jordan Basis}{Jordan Basis}
Suppose $T\in \mathscr{L}(V)$, then a basis of $V$ is called a \emph{Jordan Basis} for $T$ if the matrix form of $T$ looks like
\begin{equation*}
\begin{pmatrix}
	A_1&&0\\
	   &\ddots &\\
	0&&A_p
\end{pmatrix}
\end{equation*}
in which each $A_k$ is an upper-triangular matrix of the form
\begin{equation*}
A_k = 
\begin{pmatrix}
	\lambda_k&1&&0\\
		 &\ddots &\ddots &\\
		 &&\ddots &1\\
	0&&&\lambda_k
\end{pmatrix}
\end{equation*}
!!Different $A_k$ may have the same $\lambda_k$.
\end{definition}

\begin{theorem}{Every Nilpotent Operators has a Jordan Basis}{Every Nilpotent Operators has a Jordan Basis}
Suppose $T\in \mathscr{L}(V)$ is nilpotent, then there is a Jordan basis of $V$ for $T$.
\end{theorem}
\begin{proof}
	We'll prove this result by induction on $\dim V$. If $\dim V=1$, the result holds, obviously. Assuming that $\dim V>1$ and the result holds for all smaller dimension. Let  $m$ be the smallest number that $T^m=0$, thus there exists $u\in V$, such that $T^{m-1}u\neq 0$. Let
	\begin{equation*}
	U = \vspan (u,Tu, \ldots ,T^{m-1}u)
	\end{equation*}
	The list $u,Tu, \ldots ,T^{m-1}u$ is linear independent. (If $T^k u = a_1u + \ldots + a _{k-1} T^{k-1}$, multiply the equation by $T^{m-k}, \ldots ,T^m$ we have $a_1=\cdots =a _{k-1} = 0$, contradicting to the minimality of $m$.) If $U=V$, writing the list in reverse order gives a Jordan basis.

	Assume $U\neq V$, then we have a Jordan basis for $T|_U$, we shall find a $W$ that is invariant under $T$ and $V = U \oplus W$.

	Geometrically, a nilpotent $T$ acting on a Jordan basis is like: moving the next basis to the previous one, and deleting the first. A good way to find $W$ is to construct a space that is perpendicular to all $T^kv$.

	Let $\varphi\in V'$ be such that $\varphi(T^{m-1}u)\neq 0$, and let 
	\begin{equation*}
		W = \left\{ v\in V: \varphi(T^kv)=0,\forall k=0, \ldots ,m-1 \right\}
	\end{equation*}
	Then:
	\begin{itemize}
	\item $W$ is a subspace that is invariant under $T$. (For $\varphi(T^k(Tv))=0$.)
	\item $U+W$ is a direct sum. Suppose $v\in U\cap W,v\neq 0$, Because $v\in U$, there is $c_0, \ldots ,c_{m-1}$ such that
		\begin{equation*}
		v = c_0u + c_1Tu+\cdots +c_{m-1}T^{m-1}u.
		\end{equation*}
Let $j$ be the smallest that $c_j\neq 0$, then
\begin{equation*}
T^{m-j-1}v = c_j T^{m-1}u
\end{equation*}
Applying $\varphi$ to both sides gives a contradiction.
\item $U \oplus W=V$, our intuition is that $W$ is perpendicular to $m$ linear independent vectors, so has at most $\dim V-m$ dimension. Let $Sv = \left(\varphi(v), \ldots ,\varphi(T^{m-1}v)\right)$, then $\snull S = W$. Hence $\dim W = \dim V - \dim \range S \geq \dim V - m$. So we have $\dim (U \oplus W) \geq \dim V$.
	\end{itemize}
\end{proof}

\begin{theorem}{Jordan Form}{Jordan Form}
Suppose $\mathbb{F}=\mathbb{C}$, $T\in \mathscr{L}(V)$, then there is a Jordan basis of $V$ for $T$.
\end{theorem}
\begin{proof}
Easily proved by the previous theorem and the generalized eigenspace decomposition.
\end{proof}

\section{Trace}
\begin{definition}{Trace of a Square Matrix}{Trace of a Square Matrix}
Suppose $A$ is a square matrix in $\mathbb{F}^{n \times n}$. The trace of $A$ is defined to be the sum of the diagonal of $A$.
\end{definition}

\begin{theorem}{Trace of $AB$ and $BA$}{Trace of AB and BA}
Suppose $A\in \mathbb{F}^{m \times n}$ and $B\in \mathbb{F}^{n \times m}$, then
\begin{equation*}
\tr (AB) = \tr(BA)
\end{equation*}
\end{theorem}
\begin{proof}
Expanding the terms would do:
\begin{equation*}
\tr(AB) = \tr(BA) = \sum_{j=1}^{m} \sum_{k=1}^{n} A_{j,k}B_{k,j}
\end{equation*}
\end{proof}

\begin{theorem}{Trace do not Depend on Basis}{Trace do not Depend on Basis}
Suppose $T\in \mathscr{L}(V)$, suppose $u_k$ and $v_k$ are basis, then
\begin{equation*}
\tr \mathscr{M}(T,u_k) = \tr \mathscr{M}(T,v_k)
\end{equation*}
\end{theorem}
\begin{proof}
\begin{equation*}
\tr A  = \tr (C ^{-1}BC) = \tr (C C ^{-1}B) = \tr B
\end{equation*}
\end{proof}
This gives us confidence in defining the trace of an operator.

\begin{definition}{Trace of an Operator}{Trace of an Operator}
The trace of an Operator is the trace of any of its matrix
\end{definition}

\begin{proposition}{Properties of Trace}{Properties of Trace}
Suppose $\mathbb{F}=\mathbb{C}$ and $T\in \mathscr{L}(V)$, then
\begin{itemize}
\item $\tr T$ equals the sum of eigenvalues of $T$ including multiplicities.
	\begin{equation*}
	\tr T = d_1 \lambda_1 +\cdots +d_m \lambda_m
	\end{equation*}
\item $\tr T$ is the negative of the $z^{n-1}$ term coefficient of the characteristic polynomial.
\end{itemize}
\end{proposition}

\begin{proposition}{Trace on Inner Product Space}{Trace on Inner Product Space}
Suppose $V$ is an inner product space, and $e_1, \ldots ,e_n$ is an orthogonal basis. Then
\begin{equation*}
\tr T = \left<Te_1, e_1\right> +\cdots + \left<Te_n,e_n\right>.
\end{equation*}
\end{proposition}

\begin{theorem}{The Algebraic Characterization of Trace}{The Algebraic Characterization of Trace}
The trace is the only linear functional $\tau: \mathscr{L}(V) \rightarrow \mathbb{F}$ such that
\begin{itemize}
\item $\tau(ST) = \tau(TS)$, for all $S,T\in \mathscr{L}(V)$.
\item $\tau(I)=\dim V$.
\end{itemize}
\end{theorem}
\begin{proof}
We only need to consider $\mathbb{F}^{n \times n} \cong \mathscr{L}(V)$. Find a basis of $\mathbb{F}^{n \times n}$:
\begin{equation*}
P_{i,j} = \text{ the $(i,j)$ entry is $1$, other is $0$. }
\end{equation*}
\begin{equation*}
P_{i,j}P_{k,l}=
\begin{dcases}
	P_{i,l}, &j=k\\
	0, &j\neq k
\end{dcases}
\end{equation*}
So if $i\neq l$, we have $\tr(P_{i,l}) = \tr(P_{k,l}P_{i,j}) = 0$. Also $\tr P_{i,i} = \tr (P_{i,j}P_{j,i}) = \tr P_{j,j}$, and $\tr I = \tr P_{11}+\cdots +\tr P_{nn} = n$, so $\tr P_{i,i}=1$.
\end{proof}

\begin{example}{$AB-BA=I$}{AB-BAI}
In a finite dimensional vector space, there is no operators such that $ST-TS=I$.
\end{example}

\end{document}
